# MLflow Model Serving-Data AI Summit 2021

The easy-to-deploy turnkey solution to host machine learning (ML) models as REST endpoints that are updated automatically offers a great fit for data science teams, allowing them to completely control the end-to-end lifecycle of a real-time machine learning model from training to production.

When it comes to implementing machine learning models, data scientists must choose an approach depending on their particular use case. To a great extent, when there is a significant number of predictions to be delivered and latency is not a concern, predictions are often performed in batch, feeding the model with a significant quantity of data and then recording the predictions into a table. There are cases when models may need to be deployed in the short term (e.g. in reaction to a user action in an app), where best practise is to deploy ML models via REST endpoints. The SDK provides APIs that enable programmes to make calls to an endpoint that is always online and to obtain predictions quickly.

![MLflow](/docs/mlflow.jpg)
